{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1d93fea",
   "metadata": {},
   "source": [
    "(f_rs_ml_predict)=\n",
    "\n",
    "---------------\n",
    "```{admonition} Learning Objectives\n",
    "  - Fit and predict machine learning models to make spatial predictions\n",
    "    - Use sklearn pipelines, cross-validation and hyper parameter tuning for spatial data\n",
    "  - Predict landcover or continuous models \n",
    "  - Make predictions using timeseries data\n",
    "\n",
    "```\n",
    "```{admonition} Review\n",
    "* [Geowombat IO](f_rs_io.md)\n",
    "* [Geowombat Extraction](f_rs_extraction.md)\n",
    "* [Sklearn_xarray](https://phausamann.github.io/sklearn-xarray/)\n",
    "* [Sklearn pipelines](https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf)\n",
    "```\n",
    "--------------\n",
    "\n",
    "\n",
    "# Spatial Prediction using ML in Python\n",
    "## Create Land Use Classification using Geowombat & Sklearn\n",
    "\n",
    "The most common task for remotely sensed data is creating land cover classification. In this tutorial we will walk you through how to train a ML model using raster data. These methods are heavily dependent on the great package [sklearn_xarray](https://phausamann.github.io/sklearn-xarray/). To understand the pipeline commands please see their [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) and [examples](https://scikit-learn.org/stable/auto_examples/index.html#pipelines-and-composite-estimators). \n",
    "\n",
    "### Supervised Classification in Python\n",
    "In the following example we will use Landsat data, some training data to train a supervised sklearn model. In order to do this we first need  to have land classifications for a set of points of polygons. In this case we have three polygons with the classes ['water','crop','tree','developed']. The first step is to use `LabelEncoder` to convert these to integer based categories, which we store in a new column called 'lc'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ecbbe03",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeowombat\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgw\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeowombat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m l8_224078_20200518, l8_224078_20200518_polygons\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeowombat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fit, predict, fit_predict\n",
      "File \u001b[0;32m~/miniconda3/envs/pygis2/lib/python3.10/site-packages/geowombat/__init__.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2.1.3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mopen\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m series\n",
      "File \u001b[0;32m~/miniconda3/envs/pygis2/lib/python3.10/site-packages/geowombat/core/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transform_crs\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save\n",
      "File \u001b[0;32m~/miniconda3/envs/pygis2/lib/python3.10/site-packages/geowombat/backends/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdask_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cluster\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxarray_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m concat, mosaic\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxarray_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warp_open\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxarray_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transform_crs\n",
      "File \u001b[0;32m~/miniconda3/envs/pygis2/lib/python3.10/site-packages/geowombat/backends/xarray_.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mT\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_handler\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindows\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_window_offsets\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_filename_dates\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n",
      "File \u001b[0;32m~/miniconda3/envs/pygis2/lib/python3.10/site-packages/geowombat/core/windows.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m n_rows_cols\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrasterio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindows\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Window\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_window_offsets\u001b[39m(n_rows,\n\u001b[1;32m      7\u001b[0m                        n_cols,\n\u001b[1;32m      8\u001b[0m                        row_chunks,\n\u001b[1;32m      9\u001b[0m                        col_chunks,\n\u001b[1;32m     10\u001b[0m                        return_as\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlist\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m                        padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/pygis2/lib/python3.10/site-packages/geowombat/core/util.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_handler\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmoving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m moving_window\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pygis2/lib/python3.10/site-packages/geowombat/moving/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_moving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m moving_window\n",
      "File \u001b[0;32msrc/geowombat/moving/_moving.pyx:1\u001b[0m, in \u001b[0;36minit geowombat.moving._moving\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import geowombat as gw\n",
    "from geowombat.data import l8_224078_20200518, l8_224078_20200518_polygons\n",
    "from geowombat.ml import fit, predict, fit_predict\n",
    "import geopandas as gpd\n",
    "from sklearn_xarray.preprocessing import Featurizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# The labels are string names, so here we convert them to integers\n",
    "labels = gpd.read_file(l8_224078_20200518_polygons)\n",
    "labels['lc'] = le.fit(labels.name).transform(labels.name)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef58719",
   "metadata": {},
   "source": [
    "We are then going to generate our sklearn pipeline ([see simple tutorial here](https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf)). A pipeline simply allows us to pass a numpy array through a defined set of operations. In this case the data is passed through the following operations:\n",
    "\n",
    " * `StandardScaler`: [Normalizes](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) all variables by removing the mean and scaling to unit variance\n",
    " * `PCA`: Calculates [Principal Components](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?highlight=pca#sklearn.decomposition.PCA) to reduce dimensionality. \n",
    " * `GaussianNB`: Fits a [Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html?highlight=gaussiannb#sklearn.naive_bayes.GaussianNB) model for a quick classification. \n",
    "\n",
    " In this example we will fit and predict the model in two steps. The `fit` method returns three objects, a transformed version of the original dataset `X` that can be used by sklearn, `Xy` a tuple containing the data used for training `(X,y)` where any data outside the polygons is removed, and the trained pipeline `clf` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9761127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use a data pipeline\n",
    "pl = Pipeline([ ('scaler', StandardScaler()),\n",
    "                ('pca', PCA()),\n",
    "                ('clf', GaussianNB())])\n",
    "\n",
    "fig, ax = plt.subplots(dpi=200,figsize=(5,5))\n",
    "\n",
    "# Fit the classifier\n",
    "with gw.config.update(ref_res=150):\n",
    "    with gw.open(l8_224078_20200518, nodata=0) as src:\n",
    "        X, Xy, clf = fit(src, pl, labels, col=\"lc\")\n",
    "        y = predict(src, X, clf)\n",
    "        y.plot(robust=True, ax=ax)\n",
    "plt.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad196bb",
   "metadata": {},
   "source": [
    "In order to fit and predict to our original data in one step, we simply use `fit_predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b4d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geowombat.ml import fit_predict\n",
    "fig, ax = plt.subplots(dpi=200,figsize=(5,5))\n",
    "\n",
    "with gw.config.update(ref_res=150):\n",
    "    with gw.open(l8_224078_20200518, nodata=0) as src:\n",
    "        y = fit_predict(src, pl, labels, col='lc')\n",
    "        y.plot(robust=True, ax=ax)\n",
    "plt.tight_layout(pad=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4565cbb",
   "metadata": {},
   "source": [
    "### Unsupervised Classification in Python\n",
    "Unsupervised classification takes a different approach. Here we don't have to provide examples of different land cover types. Instead we rely on the algorithm to identify distinct clusters of similar data, and apply a unique label to each cluster. For instance, if we are talking about land cover water and trees are going to look very different. Water reflects more blue and absorbs all the near infrared, while trees reflect little blue and reflect lots of near infrared.  Therefore water and trees should 'cluster' together when plotted out according to their different blue and near infrared reflectances. These clusters will be assigned a unique value to each pixel, e.g. water will be assigned 1 and trees 2. Later, the end user will need to go back and assign the label to each numbered cluster, e.g. water=1, trees=2.\n",
    "\n",
    "In this example we will use [kmeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) to do our clustering. To run we need to decide apriori how many clusters we want to identify. Typically you want to roughly double the number of expected classes and then recombine them later into the desired labels. This helps to better understand and categorize the variation in your image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef99f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "cl = Pipeline([ ('clf', KMeans(n_clusters=6, random_state=0))])\n",
    "\n",
    "fig, ax = plt.subplots(dpi=200,figsize=(5,5))\n",
    "\n",
    "# Fit_predict unsupervised classifier\n",
    "with gw.config.update(ref_res=150):\n",
    "    with gw.open(l8_224078_20200518, nodata=0) as src:\n",
    "        y= fit_predict(src, cl)\n",
    "        y.plot(robust=True, ax=ax)\n",
    "plt.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b201e",
   "metadata": {},
   "source": [
    "In this case we can see that it effective labels different clusters of data, and now it is up to us to determine which clusters should be categorized as water, trees, and fields etc. \n",
    "\n",
    "## Spatial prediction with time series stack using Geowombat & Sklearn\n",
    "\n",
    "If you have a stack of time series data it is simple to apply the same method as we described previously, except we need to open multiple images, set `stack_dim` to 'time' and set the `time_names`.  *Note* we are just pretending we have two dates of LandSat imagery here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01312bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=200,figsize=(5,5))\n",
    "\n",
    "with gw.config.update(ref_res=150):\n",
    "   with gw.open([l8_224078_20200518, l8_224078_20200518], \n",
    "                time_names=['t1', 't2'], \n",
    "                stack_dim='time', \n",
    "                nodata=0) as src:\n",
    "        y = fit_predict(src, pl, labels, col='lc')\n",
    "        print(y)\n",
    "        # plot one time period prediction\n",
    "        y.sel(time='t1').plot(robust=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd082dd4",
   "metadata": {},
   "source": [
    "If you want to do more sophisticated model tuning using sklearn it is also possible to break up your fit and predict steps as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=200,figsize=(5,5))\n",
    "\n",
    "with gw.config.update(ref_res=150):\n",
    "    with gw.open(l8_224078_20200518, nodata=0) as src:\n",
    "        X, Xy, clf = fit(src, pl, labels, col=\"lc\")\n",
    "        y = predict(src, X, clf)\n",
    "        y.plot(robust=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f8c6f",
   "metadata": {},
   "source": [
    "## Cross-validation and Hyperparameter Tuning with Spatial Prediction\n",
    "One of the most important parts of successfully building a model is a careful assessment of model performance. To do this we will leverage some of `sklearn` built in tools. One of the most common cross-validation methods is called k-fold, where you data is broken in to independent sets of training and testing data multiple times. The ability of the model - trained on the 'training' data - to predict the outcome of the 'testing' data multiple times. We can then have a measure of how well our model will work on data it has never seen before. \n",
    "\n",
    "In this case we are going to use our supervised classification pipeline `pl` from earlier. And we will use [kfold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html?highlight=kfold#sklearn.model_selection.KFold) to do [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html). To use `kfold` with `geowombat` we need to use `CrossValidatorWrapper` as seen in the example below to allow it to work with `xarray` objects. \n",
    "\n",
    "We often also need to [hyper-parameter tune](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    "our model. In this case we will see if we need to keep 1, 2, or 3 [pca](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?highlight=pca#sklearn.decomposition.PCA) components. We might also want to experiment with whether scaling the data range impacts our perforamnce with [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standardscaler#sklearn.preprocessing.StandardScaler) by changing whether or not variables are divided by their standard deviation. \n",
    "\n",
    "To do hyper-parameter tuning with [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV) in a pipeline we need to set up the 'parameter-grid'. This part can be a little confusing. To help us let's isolate the `Pipeline` and `param_grid` from the example below:\n",
    "\n",
    "``` python\n",
    "pl = Pipeline([('scaler', StandardScaler()),\n",
    "               ('pca', PCA()),\n",
    "               ('clf', GaussianNB())])\n",
    "\n",
    "param_grid={\"scaler__with_std\":[True,False],\n",
    "            \"pca__n_components\": [1, 2, 3]\n",
    "            }\n",
    "```\n",
    "Notice that each step in the pipeline is labeled (e.g. 'scaler', 'pca', 'clf'). To try out different parameters for each step we are going to need to reference them by name in our `param_grid` dictionary. The dictionary follows this convention:\n",
    "\n",
    "`(step_name)__(parameter_name):[value_1, value2]`\n",
    "\n",
    "So `\"pca__n_components\": [1, 2, 3]` says that for the `pca` step of the pipeline, we will try out tree different values for the parameter `n_components`, allowing us to choose the one that performs best at predicting our 'testing' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e6545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn_xarray.model_selection import CrossValidatorWrapper\n",
    "\n",
    "pl = Pipeline([('scaler', StandardScaler()),\n",
    "               ('pca', PCA()),\n",
    "               ('clf', GaussianNB())])\n",
    "\n",
    "cv = CrossValidatorWrapper(KFold())\n",
    "gridsearch = GridSearchCV(pl, cv=cv, scoring='balanced_accuracy',\n",
    "                    param_grid={\n",
    "                      \"scaler__with_std\":[True,False],\n",
    "                      \"pca__n_components\": [1, 2, 3]\n",
    "                      })\n",
    "\n",
    "fig, ax = plt.subplots(dpi=200,figsize=(5,5))\n",
    "\n",
    "with gw.config.update(ref_res=150):\n",
    "    with gw.open(l8_224078_20200518, nodata=0) as src:\n",
    "        # fit a model to get Xy used to train model\n",
    "        X, Xy, pipe = fit(src, pl, labels, col=\"lc\")\n",
    "\n",
    "        # fit cross valiation and parameter tuning\n",
    "        # NOTE: must unpack * object Xy\n",
    "        gridsearch.fit(*Xy)\n",
    "        print(gridsearch.cv_results_)\n",
    "        print(gridsearch.best_score_)\n",
    "        print(gridsearch.best_params_)        \n",
    "\n",
    "        # get set tuned parameters and make the prediction\n",
    "        # Note: predict(gridsearch.best_model_) not currently supported\n",
    "        pipe.set_params(**gridsearch.best_params_)\n",
    "        y = predict(src, X, pipe)\n",
    "        y.plot(robust=True, ax=ax)\n",
    "plt.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac3141f",
   "metadata": {},
   "source": [
    "In order to create a model with the optimal parameters we need to use `gridsearch.best_params_`, which holds a dictionary of each parameter and its optimal value. To 'use' these values we need to update the parameters held in our returned pipeline, `pipe`, by using the `.set_params` method. We use `**` to unpack the dictionary values, tutorial on [unpacking here](https://medium.com/ml-and-automation/how-to-unpack-list-dictionary-tuple-in-python-c0705d29931c).\n",
    "\n",
    "Notice that the `gridsearch` has a few attributes of interest. This includes all the results of the kfold rounds `.cv_results_`, the best score obtained `.best_score_`, and the ideal set of parameters to use in the pipeline `.best_params_`.  This lase one `.best_params_` will be use to update our `pipe` pipeline for prediction. \n",
    "\n",
    "## Handling Missing Data\n",
    "\n",
    "Missing data can be a real problem when working with remote sensing data. In the case of Landsat data, missing data is often represented by a value of 0. Or perhaps you already have masked missing data values as `np.nan`. \n",
    "\n",
    "This can be a problem when using sklearn models *that expect all data to be present*. To handle this we can use the `nodata` value in `gewombat.open()` and `SimpleImputer` from sklearn in our pipeline. \n",
    "\n",
    "If we had a dataset that had 0s as missing data we could use the following to mask out 0s and replace with `np.nan`, then we can pass that data to our pipeline, that replaces `np.nan` with the mean of the column. \n",
    "\n",
    "``` python\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "classifier = Pipeline(\n",
    "    [\n",
    "        (\"remove_nan\", SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n",
    "        (\"clf\", KMeans(n_clusters=6, random_state=0)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "with gw.open(files, \n",
    "             band_names=[band_name],\n",
    "             time_names = dates,nodata=-9999  ) as ds:\n",
    "    ds = ds.gw.mask_nodata()\n",
    "    y = fit_predict(ds, classifier)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "myst": {
   "html_meta": {
    "description lang=en": "Spatial classification and prediction models. Train and fit sklearn models on raster data including LandSat or other gridded data. ",
    "description lang=es": "Modelos de clasificación y predicción espacial. Entrene y ajuste modelos sklearn en datos ráster, incluido LandSat u otros datos cuadriculados.",
    "description lang=fr": "Classification spatiale et modèles de prédiction. Entraînez et adaptez des modèles sklearn sur des données raster, y compris LandSat ou d'autres données maillées.",
    "keywords": "sklearn, spatial,raster, remote sensing, time series, landsat, sentinel",
    "property=og:locale": "en_US"
   }
  },
  "source_map": [
   17,
   46,
   63,
   72,
   89,
   92,
   102,
   109,
   121,
   128,
   140,
   144,
   152,
   179,
   214
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}