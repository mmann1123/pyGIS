{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f6e36b5",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "# Spatial Interpolation\n",
    "\n",
    "----------------\n",
    "\n",
    "```{admonition} Learning Objectives\n",
    "* Conduct various types of interpolation on point dataset\n",
    "* Obtain interpolated values at specified unsampled locations\n",
    "```\n",
    "```{admonition} Review\n",
    "* [Spatial Vector Data](c_vectors.md)\n",
    "* [Attributes & Indexing for Vector Data](e_attributes.md)\n",
    "* [Creating Spatial Vector Data](c_new_vectors.md)\n",
    "* [Merge Data & Dissolve Polygons](e_vector_merge_dissolve.md)\n",
    "```\n",
    "\n",
    "----------------\n",
    "\n",
    "Interpolation is the process of using locations with known, sampled values (of a phenomenon) to estimate the values at unknown, unsampled areas [^bolstad]. In this chapter, we will explore three interpolation methods: Thiessen polygons (Voronoi diagrams), k-nearest neighbors (KNN), and kriging.\n",
    "\n",
    "We will first begin by importing modules (click the + below to show code cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d17c5958",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/mmann1123/miniconda3/envs/pygis2/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/mmann1123/miniconda3/envs/pygis2/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/mmann1123/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/mmann1123/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/mmann1123/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/mmann1123/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/mmann1123/miniconda3/envs/pygis2/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/mmann1123/miniconda3/envs/pygis2/lib/python3.10/asyncio/base_events.py\", line 1906, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/mmann1123/miniconda3/envs/pygis2/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/mmann1123/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/mmann1123/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/mmann1123/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/mmann1123/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/mmann1123/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/mmann1123/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/mmann1123/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/mmann1123/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/mmann1123/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/mmann1123/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/mmann1123/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/mmann1123/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_2822190/679959318.py\", line 2, in <module>\n",
      "    import geopandas as gpd\n",
      "  File \"/home/mmann1123/miniconda3/envs/pygis2/lib/python3.10/site-packages/geopandas/__init__.py\", line 1, in <module>\n",
      "    from geopandas._config import options  # noqa\n",
      "  File \"/home/mmann1123/miniconda3/envs/pygis2/lib/python3.10/site-packages/geopandas/_config.py\", line 109, in <module>\n",
      "    default_value=_default_use_pygeos(),\n",
      "  File \"/home/mmann1123/miniconda3/envs/pygis2/lib/python3.10/site-packages/geopandas/_config.py\", line 95, in _default_use_pygeos\n",
      "    import geopandas._compat as compat\n",
      "  File \"/home/mmann1123/miniconda3/envs/pygis2/lib/python3.10/site-packages/geopandas/_compat.py\", line 10, in <module>\n",
      "    import shapely\n",
      "  File \"/home/mmann1123/miniconda3/envs/pygis2/lib/python3.10/site-packages/shapely/__init__.py\", line 1, in <module>\n",
      "    from .lib import GEOSException  # NOQA\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import modules\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pygis2/lib/python3.10/site-packages/geopandas/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m options  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeoseries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GeoSeries  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeodataframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GeoDataFrame  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pygis2/lib/python3.10/site-packages/geopandas/_config.py:109\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compat\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcompat\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     compat\u001b[38;5;241m.\u001b[39mset_use_pygeos(value)\n\u001b[1;32m    107\u001b[0m use_pygeos \u001b[38;5;241m=\u001b[39m Option(\n\u001b[1;32m    108\u001b[0m     key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_pygeos\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 109\u001b[0m     default_value\u001b[38;5;241m=\u001b[39m\u001b[43m_default_use_pygeos\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    110\u001b[0m     doc\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhether to use PyGEOS to speed up spatial operations. The default is True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif PyGEOS is installed, and follows the USE_PYGEOS environment variable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif set.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m     ),\n\u001b[1;32m    115\u001b[0m     validator\u001b[38;5;241m=\u001b[39m_validate_bool,\n\u001b[1;32m    116\u001b[0m     callback\u001b[38;5;241m=\u001b[39m_callback_use_pygeos,\n\u001b[1;32m    117\u001b[0m )\n\u001b[1;32m    120\u001b[0m options \u001b[38;5;241m=\u001b[39m Options({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay_precision\u001b[39m\u001b[38;5;124m\"\u001b[39m: display_precision, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_pygeos\u001b[39m\u001b[38;5;124m\"\u001b[39m: use_pygeos})\n",
      "File \u001b[0;32m~/miniconda3/envs/pygis2/lib/python3.10/site-packages/geopandas/_config.py:95\u001b[0m, in \u001b[0;36m_default_use_pygeos\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_use_pygeos\u001b[39m():\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compat\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcompat\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compat\u001b[38;5;241m.\u001b[39mUSE_PYGEOS\n",
      "File \u001b[0;32m~/miniconda3/envs/pygis2/lib/python3.10/site-packages/geopandas/_compat.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyproj\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshapely\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeos\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# pandas compat\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pygis2/lib/python3.10/site-packages/shapely/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GEOSException  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Geometry  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m geos_version, geos_version_string  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "from rasterio.plot import show\n",
    "from rasterio.transform import Affine\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "from shapely.geometry import box\n",
    "from shapely.geometry import Polygon, Point\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e77b43f",
   "metadata": {},
   "source": [
    "We will utilize shapefiles of San Francisco Bay Area county boundaries and rainfall \"values\" that were \"sampled\" in the Bay Area. We will load in the data and reproject the data (click the + below to show code cell).\n",
    "\n",
    "\n",
    "```{Note} It is critical to use a 'projected' coordinate system when doing interpolation. If you keep your data in geographic lat lon distances will vary significantly as you move up and down in latitude... since interpolation depends on distance as a way of establishing relationships this would be a problem... a big one. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e45ae",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# County boundaries\n",
    "# Source: https://opendata.mtc.ca.gov/datasets/san-francisco-bay-region-counties-clipped?geometry=-125.590%2C37.123%2C-119.152%2C38.640\n",
    "counties = gpd.read_file(\"../_static/e_vector_shapefiles/sf_bay_counties/sf_bay_counties.shp\")\n",
    "\n",
    "# Rainfall measurement \"locations\"\n",
    "# Source: https://earthworks.stanford.edu/catalog/stanford-td754wr4701\n",
    "# Modified by author by clipping raster to San Francisco Bay Area, generating random points, and extracting raster values (0-255) to the points\n",
    "rainfall = gpd.read_file(\"../_static/e_vector_shapefiles/sf_bay_rainfall/sf_bay_rainfall.shp\")\n",
    "\n",
    "# Reproject data to CA Teale Albert\n",
    "# https://nrm.dfg.ca.gov/FileHandler.ashx?DocumentID=109326&inline\n",
    "proj = \"+proj=aea +lat_1=34 +lat_2=40.5 +lat_0=0 +lon_0=-120 +x_0=0 +y_0=-4000000 +ellps=GRS80 +datum=NAD83 +units=m +no_defs \"\n",
    "counties = counties.to_crs(proj)\n",
    "rainfall = rainfall.to_crs(proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8947ddc",
   "metadata": {},
   "source": [
    "Next, we'll prepare the data for geoprocessing (click the + below to show code cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea367e",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Get X and Y coordinates of rainfall points\n",
    "x_rain = rainfall[\"geometry\"].x\n",
    "y_rain = rainfall[\"geometry\"].y\n",
    "\n",
    "# Create list of XY coordinate pairs\n",
    "coords_rain = [list(xy) for xy in zip(x_rain, y_rain)]\n",
    "\n",
    "# Get extent of counties feature\n",
    "min_x_counties, min_y_counties, max_x_counties, max_y_counties = counties.total_bounds\n",
    "\n",
    "# Get list of rainfall \"values\"\n",
    "value_rain = list(rainfall[\"VALUE\"])\n",
    "\n",
    "# Create a copy of counties dataset\n",
    "counties_dissolved = counties.copy()\n",
    "\n",
    "# Add a field with constant value of 1\n",
    "counties_dissolved[\"constant\"] = 1\n",
    "\n",
    "# Dissolve all counties to create one polygon\n",
    "counties_dissolved = counties_dissolved.dissolve(by = \"constant\").reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914928b0",
   "metadata": {},
   "source": [
    "We will also define a function for exporting rasters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3021aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_kde_raster(Z, XX, YY, min_x, max_x, min_y, max_y, proj, filename):\n",
    "    '''Export and save a kernel density raster.'''\n",
    "\n",
    "    # Get resolution\n",
    "    xres = (max_x - min_x) / len(XX)\n",
    "    yres = (max_y - min_y) / len(YY)\n",
    "\n",
    "    # Set transform\n",
    "    transform = Affine.translation(min_x - xres / 2, min_y - yres / 2) * Affine.scale(xres, yres)\n",
    "\n",
    "    # Export array as raster\n",
    "    with rasterio.open(\n",
    "            filename,\n",
    "            mode = \"w\",\n",
    "            driver = \"GTiff\",\n",
    "            height = Z.shape[0],\n",
    "            width = Z.shape[1],\n",
    "            count = 1,\n",
    "            dtype = Z.dtype,\n",
    "            crs = proj,\n",
    "            transform = transform,\n",
    "    ) as new_dataset:\n",
    "            new_dataset.write(Z, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c8b2ff",
   "metadata": {},
   "source": [
    "With any model used for prediction, it is important to assess the model fit for unobserved locations (or the accuracy of the values predicted by the model in relation to their actual values). Thus, in order to assess the fit, we break our data into two portions, a \"training\" data set used to train the model, and a \"testing\" set that remains \"unseen\" by the model but can be used to assess model performance. Effectively, we can use this \"unseen\" testing subset to validate the model because we can compare their true values with the estimated value from the model prediction.\n",
    "\n",
    "We will separate our rainfall dataset into two subsets: one for training and the other for testing. These subsets will be used in our KNN and kriging analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4254b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into testing and training sets\n",
    "coords_rain_train, coords_rain_test, value_rain_train, value_rain_test = train_test_split(coords_rain, value_rain, test_size = 0.20, random_state = 42)\n",
    "\n",
    "# Create separate GeoDataFrames for testing and training sets\n",
    "rain_train_gdf = gpd.GeoDataFrame(geometry = [Point(x, y) for x, y in coords_rain_train], crs = proj)\n",
    "rain_train_gdf[\"Actual_Value\"] = value_rain_train\n",
    "rain_test_gdf = gpd.GeoDataFrame(geometry = [Point(x, y) for x, y in coords_rain_test], crs = proj)\n",
    "rain_test_gdf[\"Actual_Value\"] = value_rain_test\n",
    "\n",
    "# Get minimum and maximum coordinate values of rainfall training points\n",
    "min_x_rain, min_y_rain, max_x_rain, max_y_rain = rain_train_gdf.total_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7199ae",
   "metadata": {},
   "source": [
    "Let's plot our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcd62ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n",
    "\n",
    "# Stylize plots\n",
    "plt.style.use('bmh')\n",
    "\n",
    "# Plot data\n",
    "counties.plot(ax = ax, color = 'bisque', edgecolor = 'dimgray')\n",
    "rain_train_gdf.plot(ax = ax, marker = 'o', color = 'limegreen', markersize = 3)\n",
    "rain_test_gdf.plot(ax = ax, marker = 'o', color = 'royalblue', markersize = 3)\n",
    "# Set title\n",
    "ax.set_title('San Francisco Bay Area - Rainfall Measurement Locations', fontdict = {'fontsize': '15', 'fontweight' : '3'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da15080",
   "metadata": {},
   "source": [
    "In the map above, the green and blue points are the rainfall points that we loaded separated into the training set and testing set, respectively.\n",
    "\n",
    "## Thiessen Polygons (Voronoi Diagrams)\n",
    "\n",
    "Thiessen polygons (also known as Voronoi diagrams) polygons allow us to perform nearest neighbor interpolation, which is perhaps the most basic type of interpolation. Thiessen polygons are be constructed around each sampled point so all the space within a specific polygon is closest in distance to that sampled point (as compared to other sampled points). Then, to perform nearest neighbor interpolation, all that space is assigned the value of that sampled point. [^bolstad]\n",
    "\n",
    "We can use the [`scipy` package](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.Voronoi.html) to create Thiessen polygons. After running the `Voronoi()` function, we can use the `vertices` attribute to get a list of vertices, which we can subsequently use to generate polygons.\n",
    "\n",
    "```{attention} When creating Thiessen polygons, the sample points toward the edges of the point shapefile's extent will have infinite Voronoi regions, because not all sides of these edge points have adjacent sample points that would constrain the regions. Consequently, these infinite regions will not be exported. To mitigate this issue, we can create dummy points well beyond the extent of our datasets, which will create finite Voronoi regions for all of our actual sample points. Then, we can clip the regions to our extent shapefile (creating dummy points far away from our actual sample points will ensure the dummy points and their infinite Voronoi regions do not interfere with the sample points and their associated finite Voronoi regions after all regions are clipped).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f7100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend extent of counties feature by using buffer\n",
    "counties_buffer = counties.buffer(100000)\n",
    "\n",
    "# Get extent of buffered input feature\n",
    "min_x_cty_tp, min_y_cty_tp, max_x_cty_tp, max_y_cty_tp = counties_buffer.total_bounds\n",
    "\n",
    "# Use extent to create dummy points and add them to list of coordinates\n",
    "coords_tp = coords_rain_train + [[min_x_cty_tp, min_y_cty_tp], [max_x_cty_tp, min_y_cty_tp],\n",
    "                                 [max_x_cty_tp, max_y_cty_tp], [min_x_cty_tp, max_y_cty_tp]]\n",
    "\n",
    "# Compute Voronoi diagram\n",
    "tp = Voronoi(coords_tp)\n",
    "\n",
    "# Create empty list of hold Voronoi polygons\n",
    "tp_poly_list = []\n",
    "\n",
    "# Create a polygon for each region\n",
    "# 'regions' attribute provides a list of indices of the vertices (in the 'vertices' attribute) that make up the region\n",
    "# Source: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.Voronoi.html\n",
    "for region in tp.regions:\n",
    "\n",
    "    # Ignore region if -1 is in the list (based on documentation)\n",
    "    if -1 in region:\n",
    "\n",
    "        # Return to top of loop\n",
    "        continue\n",
    "\n",
    "    # Otherwise, pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Check that region list has values in it\n",
    "    if len(region) != 0:\n",
    "\n",
    "        # Create a polygon by using the region list to call the correct elements in the 'vertices' attribute\n",
    "        tp_poly_region = Polygon(list(tp.vertices[region]))\n",
    "\n",
    "        # Append polygon to list\n",
    "        tp_poly_list.append(tp_poly_region)\n",
    "\n",
    "    # If no values, return to top of loop\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Create GeoDataFrame from list of polygon regions\n",
    "tp_polys = gpd.GeoDataFrame(tp_poly_list, columns = ['geometry'], crs = proj)\n",
    "\n",
    "# Clip polygon regions to the counties boundary\n",
    "tp_polys_clipped = gpd.clip(tp_polys, counties_dissolved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f0096",
   "metadata": {},
   "source": [
    "A spatial join can be conducted to assign the rainfall training \"values\" to its associated Thiessen polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a3a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If rainfall point within the polygon, assign that rainfall value to the polygon\n",
    "tp_polys_clipped_values = gpd.sjoin(rain_train_gdf, tp_polys_clipped, how = \"right\", op = 'within')\n",
    "\n",
    "# Drop un-needed column\n",
    "tp_polys_clipped_values = tp_polys_clipped_values.drop(\"index_left\", axis = 1)\n",
    "\n",
    "# Rename column\n",
    "tp_polys_clipped_values = tp_polys_clipped_values.rename(columns = {\"Actual_Value\": \"VALUE_Thiessen\"})\n",
    "\n",
    "# Display head of attribute table\n",
    "print(\"Attribute Table: Thiessen Polygon Interpolated Values\")\n",
    "display(tp_polys_clipped_values.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af88f180",
   "metadata": {},
   "source": [
    "A second spatial join can be conducted to assign those values from the Thiessen polygons to the points from the testing dataset (only if a test point falls within a polygon). We can subsequently get the out-of-sample r-squared value, which is calculated by using the data points that the model did not use (the testing dataset) and comparing the testing dataset's actual values to the values as predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If test point is within a polygon, assign that polygon's value to the test point\n",
    "rain_test_pred_tp = gpd.sjoin(rain_test_gdf, tp_polys_clipped_values, how = \"left\", op = 'within')\n",
    "\n",
    "# Drop un-needed column\n",
    "rain_test_pred_tp = rain_test_pred_tp.drop(\"index_right\", axis = 1)\n",
    "\n",
    "# Rename column\n",
    "rain_test_pred_tp = rain_test_pred_tp.rename(columns = {\"Actual_Value\": \"VALUE_Actual\", \"VALUE_Thiessen\": \"VALUE_Predict\"})\n",
    "\n",
    "# Generate out-of-sample R^2\n",
    "out_r_squared_tp = r2_score(rain_test_pred_tp.VALUE_Actual, rain_test_pred_tp.VALUE_Predict)\n",
    "print(\"Thiessen polygon out-of-sample r-squared: {}\".format(round(out_r_squared_tp, 2)))\n",
    "\n",
    "# Display attribute table\n",
    "print(\"\\nAttribute Table: Testing Dataset Interpolated Values - Thiessen Polygon Method\")\n",
    "display(rain_test_pred_tp.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da0ea8a",
   "metadata": {},
   "source": [
    "Plotting the data, we see that each polygon has one green training point (and vice-versa). All space within one polygon is closest to the known training point (green dot) within the polygon. The testing points (blue dots) are assigned the value of the Thiessen polygon in which it falls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737dcdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20, 20))\n",
    "\n",
    "# Stylize plots\n",
    "plt.style.use('bmh')\n",
    "\n",
    "# Plot data\n",
    "counties_dissolved.plot(ax = ax, color = 'none', edgecolor = 'dimgray')\n",
    "tp_polys_clipped.plot(ax = ax, cmap = 'Set3', edgecolor = 'white', linewidth = 0.5)\n",
    "rain_train_gdf.plot(ax = ax, marker = 'o', color = 'limegreen', markersize = 15)\n",
    "rain_test_pred_tp.plot(ax = ax, marker = 'o', color = 'royalblue', markersize = 15)\n",
    "\n",
    "# Iterate through each rainfall train point to add a label with its value to the plot\n",
    "for index, row in rain_train_gdf.iterrows():\n",
    "    plt.annotate(row.Actual_Value, (row.geometry.x, row.geometry.y))\n",
    "\n",
    "# Iterate through each rainfall test point to add a label with its value to the plot\n",
    "for index, row in rain_test_pred_tp.iterrows():\n",
    "    plt.annotate(row.VALUE_Predict, (row.geometry.x, row.geometry.y))\n",
    "\n",
    "# Set title\n",
    "ax.set_title('San Francisco Bay Area - Rainfall Measurement Locations & Thiessen Polygons', fontdict = {'fontsize': '15', 'fontweight' : '3'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370a37cb",
   "metadata": {},
   "source": [
    "In addition to `vertices`, there are a few other attributes we can call if we want to further explore the polygons. These attributes will provide actual values (e.g., vertices) or provide the indices for querying other attributes. [^scipy_voronoi]\n",
    "\n",
    "In the example below, we demonstrate how to extract the value of one of the Thiessen polygons at a new location for which we want a predicted value. We use the `point_region` attribute to provide the index of a point's Voronoi region, and we use that index to get the region in `regions`. That provides indices of the vertices that make up the polygon, which we use to get the appropriate values in `vertices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece6b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index for feature of interest\n",
    "feature_index_one = 5\n",
    "\n",
    "# Get a Voronoi polygon for one feature\n",
    "# 'point_region' attribute provides the index of the Voronoi region belonging to a specified point\n",
    "# Can use the index to call the appropriate element in the 'regions' attribute\n",
    "tp_poly_region_one = Polygon(tp.vertices[tp.regions[tp.point_region[feature_index_one]]])\n",
    "\n",
    "# Create GeoDataFrame for polygon\n",
    "tp_poly_region_one = gpd.GeoDataFrame([tp_poly_region_one], columns = ['geometry'], crs = proj)\n",
    "\n",
    "# Clip polygon to county boundary\n",
    "tp_poly_region_one = gpd.clip(tp_poly_region_one, counties_dissolved)\n",
    "\n",
    "# Get the equivalent feature from the rainfall dataset\n",
    "rain_one = rain_train_gdf.iloc[[feature_index_one]]\n",
    "\n",
    "# Add the rainfall value to the polygon attribute table\n",
    "tp_poly_region_one[\"VALUE_Predict\"] = rain_one[\"Actual_Value\"].values\n",
    "\n",
    "# Display attribute table\n",
    "print(\"Attribute Table: Thiessen Polygon Interpolated Value\")\n",
    "display(tp_poly_region_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bac39e",
   "metadata": {},
   "source": [
    "Here's how that one Thiessen polygon looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n",
    "\n",
    "# Stylize plots\n",
    "plt.style.use('bmh')\n",
    "\n",
    "# Plot data\n",
    "tp_poly_region_one.plot(ax = ax, color = 'lightseagreen', edgecolor = 'white', linewidth = 0.5)\n",
    "rain_one.plot(ax = ax, marker = 'o', color = 'dimgray', markersize = 100)\n",
    "\n",
    "# Set title\n",
    "ax.set_title('San Francisco Bay Area - One Point and Thiessen Polygon', fontdict = {'fontsize': '15', 'fontweight' : '3'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b014cf33",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "\n",
    "KNN (also stylized as kNN) is a neighbor-based learning method that can be used for interpolation. Unlike the Thiessen polygons method, KNN looks for a specified number `K` of sampled points closest to an unknown point. The `K` known points can be used to predict the value (discrete or continuous) of the unknown point. [^sk_nn]\n",
    "\n",
    "We can use the [`scikit-learn` module](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) to perform KNN analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of neighbors to look for\n",
    "neighbors = 5\n",
    "\n",
    "# Initialize KNN regressor\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors = neighbors, weights = \"distance\")\n",
    "\n",
    "# Fit regressor to data\n",
    "knn_regressor.fit(coords_rain_train, value_rain_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794206b",
   "metadata": {},
   "source": [
    "Now that we have created the KNN model, we can get the in-sample r-squared value. An in-sample statistic, as suggested by its name, is calculated by using the data that were used to build the model (the \"training\" dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e13ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate in-sample R^2\n",
    "in_r_squared_knn = knn_regressor.score(coords_rain_train, value_rain_train)\n",
    "print(\"KNN in-sample r-squared: {}\".format(round(in_r_squared_knn, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5d913",
   "metadata": {},
   "source": [
    "Here, the in-sample r-squared value is 100% because KNN is a \"exact interpolator.\" For exact interpolators, estimated values for known points are exactly equal to actual values. Other methods like Kriging, shown below, are inexact interpolators. For inexact interpolators, estimated values for known points are not exactly equal to actual values. Here's a [visual of inexact versus exact interpolators](https://www.onestopgis.com/GIS-Theory-and-Techniques/Spatial-Interpolation/Elements-of-Spatial-Interpolation/posts/Types-of-Spatial-Interpolation/Image-shows-Exact-interpolation-and-Inexact-interpolation.png?__cf_chl_jschl_tk__=231d8a46e00aa6fe5352048bff01af55174b6c40-1625260397-0-ARqVOIsPHXJ9wPUqWrXKldltKnf_3GrT_RVaKb7djF6cUYCwnIzVeYww8nW4e_3GiIiCNZ15eVXmksePbUz-sKbuHtD2UsSh9OWZ4w2Y2dS_zCeIMCVRUIrYCjZEEcH722m7Y2vCDGAaA6gh1RObvtBx_3Se3gCdldO6c65zunRmX4vE0jLONpdckY6MEo4YPiyZr_11QAdPLYBWwgtl3wC2XhM5mhsmVYmInXWYTiyr35sXWVn86DedDNC2rKMnvrQGo8BuBgjskX6uf_Y3tWWpYCpyghV9s0zpygqgTQYG8rNaKJyFFQdoX--tMMEYY2AwzAI2BNmAxlGlxM07tBndOjZFgphEsU9nbFhOAB-Wji4u0wwD_N97FBKdFwsNl8bGbPSpLoiIMZ94GVZqETiD29W_lFPsiunLAt3EU2Ra-pf2QC1lCaUUhUt2aCIgHH6SnzTAm-JictSfAD4cnYolpP7D27Cj_UOGLSfo2sk4KcOH9JAG3ZGF1UCkX2fBUTivVWmuYPL4l1xaGS7e0BgKuiz_TdAbeMNkyi2TtWgMzT2sZEqf5QXZzvSwbsOVf5Z8ph6imf4gaHYhhYU87yKTDkK8PnYkbuyS3zeDWnM7hTj-Gn-LtNSYpiTjA68NZA).\n",
    "\n",
    "Similarily, we can also get the out-of-sample r-squared value and compare the test dataset's actual values to the values as predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50636060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate out-of-sample R^2\n",
    "out_r_squared_knn = knn_regressor.score(coords_rain_test, value_rain_test)\n",
    "print(\"KNN out-of-sample r-squared: {}\".format(round(out_r_squared_knn, 2)))\n",
    "\n",
    "# Predict values for testing dataset\n",
    "coords_rain_test_predict_knn = knn_regressor.predict(coords_rain_test)\n",
    "\n",
    "# Create dictionary holding the actual and predicted values\n",
    "predict_dict_knn = {\"Coordinate_Pair\": coords_rain_test, \"VALUE_Actual\": value_rain_test, \"VALUE_Predict\": coords_rain_test_predict_knn}\n",
    "\n",
    "# Create dataframe from dictionary\n",
    "predict_df_knn = pd.DataFrame(predict_dict_knn)\n",
    "\n",
    "# Display attribute table\n",
    "print(\"\\nAttribute Table: Testing Set Interpolated Values - KNN Method\")\n",
    "display(predict_df_knn.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64983326",
   "metadata": {},
   "source": [
    "Out-of-sample r-squared looks pretty strong!\n",
    "\n",
    "```{tip}\n",
    "If you are just interested in identifying the `k` nearest neighbors (no interpolation), use the [`NearestNeighbors()` function](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4cf86a",
   "metadata": {},
   "source": [
    "## Kriging\n",
    "\n",
    "Kriging is a type of interpolation that uses a semivariogram, which measures spatial autocorrelation (how similar close points are in value and how this similarity changes as distance between points increases). Thus, the semivariogram determines how much influence a known point has on an unknown point as the distance between the known point and the unknown point increases. In other words, the weight of a known point on an unknown point decreases with increasing distance, and the semivariogram determines how quickly that weight tapers with increasing distance. [^bolstad], [^esri_kriging]\n",
    "\n",
    "For more information, see this [ArcGIS help guide on kriging](https://pro.arcgis.com/en/pro-app/latest/tool-reference/3d-analyst/how-kriging-works.htm).\n",
    "\n",
    "Two Python packages that can be used for kriging include `scikit-learn` and `pykrige`. The former package works best when the input data has a WGS 84 projection, so we will begin by reprojecting all of our data to that coordinate system (click the + below to show code cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a041de8",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Set projection to WGS 84 and reproject data\n",
    "proj_wgs = 4326\n",
    "counties_wgs = counties.to_crs(proj_wgs)\n",
    "rainfall_wgs = rainfall.to_crs(proj_wgs)\n",
    "rain_train_gdf_wgs = rain_train_gdf.to_crs(proj_wgs)\n",
    "rain_test_gdf_wgs = rain_test_gdf.to_crs(proj_wgs)\n",
    "\n",
    "# Get X and Y coordinates of rainfall points\n",
    "x_rain_wgs = rainfall_wgs[\"geometry\"].x\n",
    "y_rain_wgs = rainfall_wgs[\"geometry\"].y\n",
    "\n",
    "# Create list of XY coordinate pairs\n",
    "coords_rain_train_wgs = [list(xy) for xy in zip(rain_train_gdf_wgs[\"geometry\"].x, rain_train_gdf_wgs[\"geometry\"].y)]\n",
    "coords_rain_test_wgs = [list(xy) for xy in zip(rain_test_gdf_wgs[\"geometry\"].x, rain_test_gdf_wgs[\"geometry\"].y)]\n",
    "\n",
    "# Get minimum and maximum coordinate values of rainfall points\n",
    "min_x_rain_wgs, min_y_rain_wgs, max_x_rain_wgs, max_y_rain_wgs = rain_train_gdf_wgs.total_bounds\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad1eb71",
   "metadata": {},
   "source": [
    "### Method 1 - Using `PyKrige`\n",
    "\n",
    "The [`pykrige` module](https://geostat-framework.readthedocs.io/projects/pykrige/en/stable/index.html) offers ordinary and universal kriging. It also supports various variogram models in addition to Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f13fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://geostat-framework.readthedocs.io/projects/pykrige/en/latest/examples/04_krige_geometric.html\n",
    "\n",
    "# Create a 100 by 100 grid\n",
    "# Horizontal and vertical cell counts should be the same\n",
    "XX_pk_krig = np.linspace(min_x_rain, max_x_rain, 100)\n",
    "YY_pk_krig = np.linspace(min_y_rain, max_y_rain, 100)\n",
    "\n",
    "# Generate ordinary kriging object\n",
    "OK = OrdinaryKriging(\n",
    "    np.array(x_rain),\n",
    "    np.array(y_rain),\n",
    "    value_rain,\n",
    "    variogram_model = \"linear\",\n",
    "    verbose = False,\n",
    "    enable_plotting = False,\n",
    "    coordinates_type = \"euclidean\",\n",
    ")\n",
    "\n",
    "# Evaluate the method on grid\n",
    "Z_pk_krig, sigma_squared_p_krig = OK.execute(\"grid\", XX_pk_krig, YY_pk_krig)\n",
    "\n",
    "# Export raster\n",
    "export_kde_raster(Z = Z_pk_krig, XX = XX_pk_krig, YY = YY_pk_krig,\n",
    "                  min_x = min_x_rain, max_x = max_x_rain, min_y = min_y_rain, max_y = max_y_rain,\n",
    "                  proj = proj, filename = \"../temp/e_bay-area-rain_pk_kriging.tif\")\n",
    "\n",
    "# Open raster\n",
    "raster_pk = rasterio.open(\"../temp/e_bay-area-rain_pk_kriging.tif\")\n",
    "\n",
    "\n",
    "# Create polygon with extent of raster\n",
    "poly_shapely = box(*raster_pk.bounds)\n",
    "\n",
    "# Create a dictionary with needed attributes and required geometry column\n",
    "attributes_df = {'Attribute': ['name1'], 'geometry': poly_shapely}\n",
    "\n",
    "# Convert shapely object to a GeoDataFrame\n",
    "raster_pk_extent = gpd.GeoDataFrame(attributes_df, geometry = 'geometry', crs = proj)\n",
    "\n",
    "# Create copy of test dataset\n",
    "rain_test_gdf_pk_krig = rain_test_gdf.copy()\n",
    "\n",
    "# Subset the GeoDataFrame by checking which test points are within the raster extent polygon\n",
    "# If a test point is beyond the extent of training points dataset, the kriging output may not cover that test point\n",
    "rain_test_gdf_pk_krig = rain_test_gdf_pk_krig[rain_test_gdf_pk_krig.within(raster_pk_extent.geometry.values[0])]\n",
    "\n",
    "# Create list of XY coordinate pairs for the test points that fall within raster extent polygon\n",
    "coords_rain_test_pk_krig = [list(xy) for xy in zip(rain_test_gdf_pk_krig[\"geometry\"].x, rain_test_gdf_pk_krig[\"geometry\"].y)]\n",
    "\n",
    "# Extract raster value at each test point and add the values to the GeoDataFrame\n",
    "rain_test_gdf_pk_krig[\"VALUE_Predict\"] = [x[0] for x in raster_pk.sample(coords_rain_test_pk_krig)]\n",
    "\n",
    "# Generate out-of-sample R^2\n",
    "out_r_squared_tp = r2_score(rain_test_gdf_pk_krig.Actual_Value, rain_test_gdf_pk_krig.VALUE_Predict)\n",
    "print(\"PyKrige Kriging out-of-sample r-squared: {}\".format(round(out_r_squared_tp, 2)))\n",
    "\n",
    "# Display attribute table\n",
    "print(\"\\nAttribute Table: Random Points Interpolated Values - PyKrige Kriging Method\")\n",
    "display(rain_test_gdf_pk_krig.head(2))\n",
    "\n",
    "\n",
    "# Mask raster to counties shape\n",
    "out_image_pk, out_transform_pk = rasterio.mask.mask(raster_pk, counties.geometry.values, crop = True)\n",
    "\n",
    "# Stylize plots\n",
    "plt.style.use('bmh')\n",
    "\n",
    "# Plot data\n",
    "fig, ax = plt.subplots(1, figsize = (10, 10))\n",
    "show(out_image_pk, ax = ax, transform = out_transform_pk, cmap = \"RdPu\")\n",
    "ax.plot(x_rain, y_rain, 'k.', markersize = 2, alpha = 0.5)\n",
    "counties.plot(ax = ax, color = 'none', edgecolor = 'dimgray')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Set title\n",
    "ax.set_title('San Francisco Bay Area - Interpolating Rainfall using Kriging from PyKrige', fontdict = {'fontsize': '15', 'fontweight' : '3'})\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67384938",
   "metadata": {},
   "source": [
    "### Method 2- Using `scikit-learn`\n",
    "\n",
    "Kriging can be performed using [Gaussian processes from the `scikit-learn` module](https://scikit-learn.org/stable/modules/gaussian_process.html) (Gaussian processes is essentially equivalent to kriging). Various kernels for Gaussian processes can be specified. We will continue to use the training and testing datasets created from our KNN analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ad0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Create a 100 by 100 cell mesh grid\n",
    "# Horizontal and vertical cell counts should be the same\n",
    "XX_sk_krig, YY_sk_krig = np.mgrid[min_x_rain_wgs:max_x_rain_wgs:100j, min_y_rain_wgs:max_y_rain_wgs:100j]\n",
    "\n",
    "# Create 2-D array of the coordinates (paired) of each cell in the mesh grid\n",
    "positions_sk_krig = np.vstack([XX_sk_krig.ravel(), YY_sk_krig.ravel()]).T\n",
    "\n",
    "# Generate Gaussian Process model (can change parameters as desired)\n",
    "gp = GaussianProcessRegressor(n_restarts_optimizer = 10)\n",
    "\n",
    "# Fit kernel density estimator to coordinates and values\n",
    "gp.fit(coords_rain_train_wgs, value_rain_train)\n",
    "\n",
    "# Evaluate the model on coordinate pairs\n",
    "Z_sk_krig = gp.predict(positions_sk_krig)\n",
    "\n",
    "# Reshape the data to fit mesh grid\n",
    "Z_sk_krig = Z_sk_krig.reshape(XX_sk_krig.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbaf441",
   "metadata": {},
   "source": [
    "Next, we can calculate our r-squared statistics and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Generate in-sample R^2\n",
    "in_r_squared_sk_krig = gp.score(coords_rain_train_wgs, value_rain_train)\n",
    "print(\"Scikit-Learn Kriging in-sample r-squared: {}\".format(round(in_r_squared_sk_krig, 2)))\n",
    "\n",
    "# Generate out-of-sample R^2\n",
    "out_r_squared_sk_krig = gp.score(coords_rain_test_wgs, value_rain_test)\n",
    "print(\"Scikit-Learn Kriging out-of-sample r-squared: {}\".format(round(out_r_squared_sk_krig, 2)))\n",
    "\n",
    "# Predict values for testing dataset\n",
    "coords_rain_test_predict_sk_krig = gp.predict(coords_rain_test_wgs)\n",
    "\n",
    "# Create dictionary holding the actual and predicted values\n",
    "predict_dict_sk_krig = {\"Coordinate_Pair\": coords_rain_test_wgs, \"VALUE_Actual\": value_rain_test, \"VALUE_Predict\": coords_rain_test_predict_sk_krig}\n",
    "\n",
    "# Create dataframe from dictionary\n",
    "predict_df_sk_krig = pd.DataFrame(predict_dict_sk_krig)\n",
    "\n",
    "# Display attribute table\n",
    "print(\"\\nAttribute Table: Testing Set Interpolated Values - Scikit-Learn Kriging Method\")\n",
    "display(predict_df_sk_krig.head(2))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f2ef7",
   "metadata": {},
   "source": [
    "Model seems like a good fit! Let's export the raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d232a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Flip array vertically and rotate 270 degrees\n",
    "Z_sk_krig = np.rot90(np.flip(Z_sk_krig, 0), 3)\n",
    "\n",
    "# Export raster\n",
    "export_kde_raster(Z = Z_sk_krig, XX = XX_sk_krig, YY = YY_sk_krig,\n",
    "                  min_x = min_x_rain_wgs, max_x = max_x_rain_wgs, min_y = min_y_rain_wgs, max_y = max_y_rain_wgs,\n",
    "                  proj = proj_wgs, filename = ../temp/e_bay-area-rain_sk_kriging.tif\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1bc2f3",
   "metadata": {},
   "source": [
    "```{attention} The resulting raster should be clipped. Because the resulting raster covers the extent of the points in a bounding box fashion, the raster in this case covers areas that are not within the counties boundaries (such as in the ocean) where we do not have sample points. Thus, there will be interpolated values in those areas that might not make sense.\n",
    "```\n",
    "\n",
    "Finally, we import the raster, mask it to the counties boundaries, and plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9258c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Open raster\n",
    "raster_sk = rasterio.open(\"../temp/e_bay-area-rain_sk_kriging.tif\")\n",
    "\n",
    "# Mask raster to counties shape\n",
    "out_image_sk, out_transform_sk = rasterio.mask.mask(raster_sk, counties_wgs.geometry.values, crop = True)\n",
    "\n",
    "# Stylize plots\n",
    "plt.style.use('bmh')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a856d167",
   "metadata": {},
   "source": [
    "### Method 2- Using `scikit-learn`\n",
    "\n",
    "Kriging can be performed using [Gaussian processes from the `scikit-learn` module](https://scikit-learn.org/stable/modules/gaussian_process.html) (Gaussian processes is essentially equivalent to kriging). Various kernels for Gaussian processes can be specified. We will continue to use the training and testing datasets created from our KNN analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbffa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Create a 100 by 100 cell mesh grid\n",
    "# Horizontal and vertical cell counts should be the same\n",
    "XX_sk_krig, YY_sk_krig = np.mgrid[min_x_rain_wgs:max_x_rain_wgs:100j, min_y_rain_wgs:max_y_rain_wgs:100j]\n",
    "\n",
    "# Create 2-D array of the coordinates (paired) of each cell in the mesh grid\n",
    "positions_sk_krig = np.vstack([XX_sk_krig.ravel(), YY_sk_krig.ravel()]).T\n",
    "\n",
    "# Generate Gaussian Process model (can change parameters as desired)\n",
    "gp = GaussianProcessRegressor(n_restarts_optimizer = 10)\n",
    "\n",
    "# Fit kernel density estimator to coordinates and values\n",
    "gp.fit(coords_rain_train_wgs, value_rain_train)\n",
    "\n",
    "# Evaluate the model on coordinate pairs\n",
    "Z_sk_krig = gp.predict(positions_sk_krig)\n",
    "\n",
    "# Reshape the data to fit mesh grid\n",
    "Z_sk_krig = Z_sk_krig.reshape(XX_sk_krig.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b463d",
   "metadata": {},
   "source": [
    "Next, we can calculate our r-squared statistics and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3922244",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Generate in-sample R^2\n",
    "in_r_squared_sk_krig = gp.score(coords_rain_train_wgs, value_rain_train)\n",
    "print(\"Scikit-Learn Kriging in-sample r-squared: {}\".format(round(in_r_squared_sk_krig, 2)))\n",
    "\n",
    "# Generate out-of-sample R^2\n",
    "out_r_squared_sk_krig = gp.score(coords_rain_test_wgs, value_rain_test)\n",
    "print(\"Scikit-Learn Kriging out-of-sample r-squared: {}\".format(round(out_r_squared_sk_krig, 2)))\n",
    "\n",
    "# Predict values for testing dataset\n",
    "coords_rain_test_predict_sk_krig = gp.predict(coords_rain_test_wgs)\n",
    "\n",
    "# Create dictionary holding the actual and predicted values\n",
    "predict_dict_sk_krig = {\"Coordinate_Pair\": coords_rain_test_wgs, \"VALUE_Actual\": value_rain_test, \"VALUE_Predict\": coords_rain_test_predict_sk_krig}\n",
    "\n",
    "# Create dataframe from dictionary\n",
    "predict_df_sk_krig = pd.DataFrame(predict_dict_sk_krig)\n",
    "\n",
    "# Display attribute table\n",
    "print(\"\\nAttribute Table: Testing Set Interpolated Values - Scikit-Learn Kriging Method\")\n",
    "display(predict_df_sk_krig.head(2))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fe2199",
   "metadata": {},
   "source": [
    "Model seems like a good fit! Let's export the raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf4c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Flip array vertically and rotate 270 degrees\n",
    "Z_sk_krig = np.rot90(np.flip(Z_sk_krig, 0), 3)\n",
    "\n",
    "# Export raster\n",
    "export_kde_raster(Z = Z_sk_krig, XX = XX_sk_krig, YY = YY_sk_krig,\n",
    "                  min_x = min_x_rain_wgs, max_x = max_x_rain_wgs, min_y = min_y_rain_wgs, max_y = max_y_rain_wgs,\n",
    "                  proj = proj_wgs, filename = ../temp/e_bay-area-rain_sk_kriging.tif\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab30c71",
   "metadata": {},
   "source": [
    "```{attention} The resulting raster should be clipped. Because the resulting raster covers the extent of the points in a bounding box fashion, the raster in this case covers areas that are not within the counties boundaries (such as in the ocean) where we do not have sample points. Thus, there will be interpolated values in those areas that might not make sense.\n",
    "```\n",
    "\n",
    "Finally, we import the raster, mask it to the counties boundaries, and plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adad5759",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Open raster\n",
    "raster_sk = rasterio.open(\"../temp/e_bay-area-rain_sk_kriging.tif\")\n",
    "\n",
    "# Mask raster to counties shape\n",
    "out_image_sk, out_transform_sk = rasterio.mask.mask(raster_sk, counties_wgs.geometry.values, crop = True)\n",
    "\n",
    "# Stylize plots\n",
    "plt.style.use('bmh')\n",
    "\n",
    "# Plot data\n",
    "fig, ax = plt.subplots(1, figsize = (10, 10))\n",
    "show(out_image_sk, ax = ax, transform = out_transform_sk, cmap = \"RdPu\")\n",
    "ax.plot(x_rain_wgs, y_rain_wgs, 'k.', markersize = 2, alpha = 0.5)\n",
    "counties_wgs.plot(ax = ax, color = 'none', edgecolor = 'dimgray')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Set title\n",
    "ax.set_title('San Francisco Bay Area - Interpolating Rainfall using Kriging from Scikit-Learn', fontdict = {'fontsize': '15', 'fontweight' : '3'})\n",
    "\n",
    "# Display plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot data\n",
    "fig, ax = plt.subplots(1, figsize = (10, 10))\n",
    "show(out_image_sk, ax = ax, transform = out_transform_sk, cmap = \"RdPu\")\n",
    "ax.plot(x_rain_wgs, y_rain_wgs, 'k.', markersize = 2, alpha = 0.5)\n",
    "counties_wgs.plot(ax = ax, color = 'none', edgecolor = 'dimgray')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Set title\n",
    "ax.set_title('San Francisco Bay Area - Interpolating Rainfall using Kriging from Scikit-Learn', fontdict = {'fontsize': '15', 'fontweight' : '3'})\n",
    "\n",
    "# Display plot\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87afdf35",
   "metadata": {},
   "source": [
    "[^bolstad]: GIS Fundamentals: A First Text on Geographic Information Systems, 5th ed., Paul Bolstad\n",
    "[^scipy_voronoi]: [scipy.spatial.Voronoi, SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.Voronoi.html)\n",
    "[^sk_nn]: [Nearest Neighbors, scikit-learn](https://scikit-learn.org/stable/modules/neighbors.html)\n",
    "[^esri_kriging]: [How Kriging works, Esri](https://pro.arcgis.com/en/pro-app/latest/tool-reference/3d-analyst/how-kriging-works.htm)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "myst": {
   "html_meta": {
    "description lang=en": "Learn how to interpolate spatial data using python. Interpolation is the process of using locations with known, sampled values (of a phenomenon) to estimate the values at unknown, unsampled areas.",
    "keywords": "python, spatial, interpolation, vector, shapefile",
    "property=og:locale": "en_US"
   }
  },
  "source_map": [
   15,
   39,
   59,
   67,
   86,
   90,
   114,
   118,
   142,
   148,
   160,
   164,
   177,
   190,
   240,
   244,
   257,
   261,
   278,
   282,
   305,
   311,
   335,
   339,
   352,
   360,
   369,
   373,
   377,
   383,
   400,
   408,
   418,
   440,
   446,
   526,
   533,
   554,
   558,
   581,
   585,
   595,
   602,
   613,
   618,
   639,
   643,
   666,
   670,
   680,
   687,
   725
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}