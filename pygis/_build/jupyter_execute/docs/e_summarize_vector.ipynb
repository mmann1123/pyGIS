{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e71719",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "```{admonition} Learning Objectives\n",
    "* Create a grid to bin features\n",
    "* Bin features using the grid\n",
    "* Display kernel density estimation results and export resulting raster\n",
    "```\n",
    "```{admonition} Review\n",
    "* [Geospatial Vector Data](c_vectors.md)\n",
    "* [Geospatial Raster Data](c_rasters.md)\n",
    "* [Attributes & Indexing for Vector Data](e_attributes.md)\n",
    "* [Creating Geospatial Vector Data](e_new_vectors.md)\n",
    "```\n",
    "\n",
    "# Summary Operations to Depict Concentrations\n",
    "\n",
    "Summary operations are useful for aggregating data, whether it be for analyzing overall trends or visualizing concentrations of data. Summarizing allows for effective analysis and communication of the data as compared to simply looking at or displaying points, lines, and polygons on a map.\n",
    "\n",
    "This chapter will explore two summary operations that highlight concentrations of data: summarize by grid (count points in a grid) and kernel density.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we will import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0dbb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon, box\n",
    "import math\n",
    "import geoplot as gplt\n",
    "from scipy import stats\n",
    "import rasterio\n",
    "from rasterio.transform import Affine\n",
    "from sklearn.datasets import fetch_species_distributions\n",
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ea2bc",
   "metadata": {},
   "source": [
    "We will utilize shapefiles of San Francisco Bay Area county boundaries and wells within the Bay Area and the surrounding 50 km. We will load in the data and reproject the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# County boundaries\n",
    "# Source: https://opendata.mtc.ca.gov/datasets/san-francisco-bay-region-counties-clipped?geometry=-125.590%2C37.123%2C-119.152%2C38.640\n",
    "counties = gpd.read_file(\"../_static/e_vector_shapefiles/sf_bay_counties/sf_bay_counties.shp\")\n",
    "\n",
    "# Well locations\n",
    "# Source: https://gis.data.ca.gov/datasets/3a3e681b894644a9a95f9815aeeeb57f_0?geometry=-123.143%2C36.405%2C-119.230%2C37.175\n",
    "# Modified by author so that only the well locations within the counties and the surrounding 50 km were kept\n",
    "wells = gpd.read_file(\"../_static/e_vector_shapefiles/sf_bay_wells_50km/sf_bay_wells_50km.shp\")\n",
    "\n",
    "# Reproject data to NAD83(HARN) / California Zone 3\n",
    "# https://spatialreference.org/ref/epsg/2768/\n",
    "proj = 2768\n",
    "counties = counties.to_crs(proj)\n",
    "wells = wells.to_crs(proj)\n",
    "\n",
    "# Create a column that assigns each well a number\n",
    "wells[\"Well_ID\"] = np.arange(wells.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb052bc0",
   "metadata": {},
   "source": [
    "## Summarize by Grid\n",
    "\n",
    "To summarize by grid, we create a new polygon layer consisting of a grid and overlay on another feature. We can summarize an aspect of that feature within each cell of the grid. The polygon layer commonly consists of a fishnet (rectangular cells), but using hexagons as a grid is becoming increasingly widespread.\n",
    "\n",
    "Let's define a function that will create a grid of either rectangles or hexagons of a specified side length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e45a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid(feature, shape, side_length):\n",
    "    '''Create a grid consisting of either rectangles or hexagons with a specified side length that covers the extent of input feature.'''\n",
    "\n",
    "    # Get extent of input feature\n",
    "    min_x, min_y, max_x, max_y = feature.total_bounds\n",
    "\n",
    "    # Slightly displace the minimum and maximum values of the feature extent\n",
    "    # This decreases likelihood that a feature will fall directly on a cell boundary (in between two cells)\n",
    "    min_x -= abs(min_x) * 0.0005\n",
    "    min_y -= abs(min_y) * 0.0005\n",
    "    max_x += abs(max_x) * 0.0005\n",
    "    max_y += abs(max_y) * 0.0005\n",
    "\n",
    "    # Create empty list to hold individual cells that will make up the grid\n",
    "    cells_list = []\n",
    "\n",
    "    # Create grid of squares if specified\n",
    "    if shape in [\"square\", \"rectangle\", \"box\"]:\n",
    "\n",
    "        # Adapted from https://james-brennan.github.io/posts/fast_gridding_geopandas/\n",
    "        # Create and iterate through list of x values that will define column positions with specified side length\n",
    "        for x in np.arange(min_x - side_length, max_x + side_length, side_length):\n",
    "\n",
    "            # Create and iterate through list of y values that will define row positions with specified side length\n",
    "            for y in np.arange(min_y - side_length, max_y + side_length, side_length):\n",
    "\n",
    "                # Create a box with specified side length and append to list\n",
    "                cells_list.append(box(x, y, x + side_length, y + side_length))\n",
    "\n",
    "\n",
    "    # Otherwise, create grid of hexagons\n",
    "    elif shape == \"hexagon\":\n",
    "\n",
    "        # Set horizontal displacement that will define column positions with specified side length (based on normal hexagon)\n",
    "        x_step = 1.5 * side_length\n",
    "\n",
    "        # Set vertical displacement that will define row positions with specified side length (based on normal hexagon)\n",
    "        # This is the distance between the centers of two hexagons stacked on top of each other (vertically)\n",
    "        y_step = math.sqrt(3) * side_length\n",
    "\n",
    "        # Get apothem (distance between center and midpoint of a side, based on normal hexagon)\n",
    "        apothem = (math.sqrt(3) * side_length / 2)\n",
    "\n",
    "        # Set column number\n",
    "        column_number = 0\n",
    "\n",
    "        # Create and iterate through list of x values that will define column positions with vertical displacement\n",
    "        for x in np.arange(min_x, max_x + x_step, x_step):\n",
    "\n",
    "            # Create and iterate through list of y values that will define column positions with horizontal displacement\n",
    "            for y in np.arange(min_y, max_y + y_step, y_step):\n",
    "\n",
    "                # Create hexagon with specified side length\n",
    "                hexagon = [[x + math.cos(math.radians(angle)) * side_length, y + math.sin(math.radians(angle)) * side_length] for angle in range(0, 360, 60)]\n",
    "\n",
    "                # Append hexagon to list\n",
    "                cells_list.append(Polygon(hexagon))\n",
    "\n",
    "            # Check if column number is even\n",
    "            if column_number % 2 == 0:\n",
    "\n",
    "                # If even, expand minimum and maximum y values by apothem value to vertically displace next row\n",
    "                # Expand values so as to not miss any features near the feature extent\n",
    "                min_y -= apothem\n",
    "                max_y += apothem\n",
    "\n",
    "            # Else, odd\n",
    "            else:\n",
    "\n",
    "                # Revert minimum and maximum y values back to original\n",
    "                min_y += apothem\n",
    "                max_y -= apothem\n",
    "\n",
    "            # Increase column number by 1\n",
    "            column_number += 1\n",
    "\n",
    "    # Else, raise error\n",
    "    else:\n",
    "        raise Exception(\"Specify a rectangle or hexagon as the grid shape.\")\n",
    "\n",
    "    # Create grid from list of cells\n",
    "    grid = gpd.GeoDataFrame(cells_list, columns = ['geometry'], crs = proj)\n",
    "\n",
    "    # Create a column that assigns each grid a number\n",
    "    grid[\"Grid_ID\"] = np.arange(len(grid))\n",
    "\n",
    "    # Return grid\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba7d488",
   "metadata": {},
   "source": [
    "We will illustrate this methodology by counting the number of well points within each cell of the grid. There are two different ways we can accomplish this methodology, both with advantages and disadvantages.\n",
    "\n",
    "To begin, we will set some global parameters for both examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997b24d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set side length for cells in grid\n",
    "# This is dependent on projection chosen as length is in units specified in projection\n",
    "side_length = 5000\n",
    "\n",
    "# Set shape of grid\n",
    "shape = \"hexagon\"\n",
    "# shape = \"rectangle\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfe3444",
   "metadata": {},
   "source": [
    "### Method 1 - Group by\n",
    "\n",
    "This method involves using spatial joins to allocate each point to the cell in which it resides. All the points within each cell are subsequently grouped together and counted.\n",
    "\n",
    "First, we will create a grid over the Bay Area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c166e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid\n",
    "bay_area_grid = create_grid(feature = wells, shape = shape, side_length = side_length)\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n",
    "\n",
    "# Plot data\n",
    "counties.plot(ax = ax, color = 'bisque', edgecolor = 'dimgray')\n",
    "wells.plot(ax = ax, marker = 'o', color = 'dodgerblue', markersize = 3)\n",
    "bay_area_grid.plot(ax = ax, color = 'none', edgecolor = 'lightseagreen', alpha = 0.55)\n",
    "\n",
    "# Set title\n",
    "ax.set_title('San Francisco Bay Area - Boundaries, Wells, and Grids', fontdict = {'fontsize': '15', 'fontweight' : '3'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db4ec3",
   "metadata": {},
   "source": [
    "Next, we will conduct a spatial join for each well point, essentially assigning it to a cell. We can add a field with a value of `1`, group all the wells in a cell, and aggregate all those `1` values to get the total number of wells in a cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216bc5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform spatial join, merging attribute table of wells point and that of the cell with which it intersects\n",
    "# op = \"intersects\" also counts those that fall on a cell boundary (between two cells)\n",
    "# op = \"within\" will not count those fall on a cell boundary\n",
    "wells_cell = gpd.sjoin(wells, bay_area_grid, how = \"inner\", op = \"intersects\")\n",
    "\n",
    "# Remove duplicate counts\n",
    "# With intersect, those that fall on a boundary will be allocated to all cells that share that boundary\n",
    "wells_cell = wells_cell.drop_duplicates(subset = ['Well_ID']).reset_index(drop = True)\n",
    "\n",
    "# Set field name to hold count value\n",
    "count_field = \"Count\"\n",
    "\n",
    "# Add a field with constant value of 1\n",
    "wells_cell[count_field] = 1\n",
    "\n",
    "# Group GeoDataFrame by cell while aggregating the Count values\n",
    "wells_cell = wells_cell.groupby('Grid_ID').agg({count_field:'sum'})\n",
    "\n",
    "# Merge the resulting grouped dataframe with the grid GeoDataFrame, using a left join to keep all cell polygons\n",
    "bay_area_grid = bay_area_grid.merge(wells_cell, on = 'Grid_ID', how = \"left\")\n",
    "\n",
    "# Fill the NaN values (cells without any points) with 0\n",
    "bay_area_grid[count_field] = bay_area_grid[count_field].fillna(0)\n",
    "\n",
    "# Convert Count field to integer\n",
    "bay_area_grid[count_field] = bay_area_grid[count_field].astype(int)\n",
    "\n",
    "# Display grid attribute table\n",
    "display(bay_area_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df2b64",
   "metadata": {},
   "source": [
    "We can plot the data to see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2edc143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n",
    "\n",
    "# Plot data\n",
    "counties.plot(ax = ax, color = 'none', edgecolor = 'dimgray')\n",
    "wells.plot(ax = ax, marker = 'o', color = 'dimgray', markersize = 3)\n",
    "bay_area_grid.plot(ax = ax, column = \"Count\", cmap = \"RdPu\", edgecolor = 'lightseagreen', linewidth = 0.5, alpha = 0.70, legend = True)\n",
    "\n",
    "# Set title\n",
    "ax.set_title('San Francisco Bay Area - Binning Well Points', fontdict = {'fontsize': '15', 'fontweight' : '3'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c751d26",
   "metadata": {},
   "source": [
    "The advantage of this method is that it is pretty fast. To verify that all points have been counted once, we can check the aggregate of all the point sums for each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f1ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check total number of well points counted and compare to number of well points in input data\n",
    "print(\"Total number of well points counted: {}\\nNumber of well points in input data: {}\".format(sum(bay_area_grid.Count), len(wells)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b18243",
   "metadata": {},
   "source": [
    "### Method 2 - Iterate through each feature\n",
    "\n",
    "This second method is slightly more intuitive, but it can take a long time to run. We will use a subset of the input data--those that fall within Santa Clara County--to illustrate this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ebb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the Santa Clara County boundary\n",
    "sc_county = counties[counties[\"coname\"] == \"Santa Clara County\"]\n",
    "\n",
    "# Multiply the boundary dataset by the number of rows in the wells dataset, and concatenate all the boundary datasets together into one dataframe\n",
    "sc_county_replicate_df = pd.concat([sc_county] * wells.shape[0], ignore_index = True)\n",
    "\n",
    "# Create a GeoDataFrame from the county dataframe\n",
    "sc_county_replicate_gdf = gpd.GeoDataFrame(sc_county_replicate_df)\n",
    "\n",
    "# Subset the GeoDataFrame by checking which wells are within Santa Clara County\n",
    "sc_county_wells = wells[wells.within(sc_county_replicate_gdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca0217",
   "metadata": {},
   "source": [
    "First, we will create a grid over Santa Clara County."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5fe8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid\n",
    "sc_county_grid = create_grid(feature = sc_county_wells, shape = shape, side_length = side_length)\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n",
    "\n",
    "# Plot data\n",
    "sc_county.plot(ax = ax, color = 'bisque', edgecolor = 'dimgray')\n",
    "sc_county_wells.plot(ax = ax, marker = 'o', color = 'dodgerblue', markersize = 3)\n",
    "sc_county_grid.plot(ax = ax, color = 'none', edgecolor = 'lightseagreen', alpha = 0.55)\n",
    "\n",
    "# Set title\n",
    "ax.set_title('Santa Clara County - Boundaries, Wells, and Grids', fontdict = {'fontsize': '15', 'fontweight' : '3'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdf2e78",
   "metadata": {},
   "source": [
    "We iterate through each cell in the grid and set a counter for each cell. We iterate through each well point and see if it is within (or intersects) the cell. If it is, the counter is increased by 1, and the feature is \"discarded\" so that it won't be counted again (resolving the issue of a point falling on the boundary between two cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8174a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list used to hold count values for each grid\n",
    "counts_list = []\n",
    "\n",
    "# Create empty list to hold index of points that have already been matched to a grid\n",
    "counted_points = []\n",
    "\n",
    "# Iterate through each cell in grid\n",
    "for i_1 in range(0, sc_county_grid.shape[0]):\n",
    "\n",
    "    # Get a cell by index\n",
    "    cell = sc_county_grid.iloc[[i_1]]\n",
    "\n",
    "    # Reset index of cell to 0\n",
    "    cell = cell.reset_index(drop = True)\n",
    "\n",
    "    # Set point count to 0\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through each feature in wells dataset\n",
    "    for i_2 in range(0, sc_county_wells.shape[0]):\n",
    "\n",
    "        # Check if index of point is in list of indices whose points have already been matched to a grid and counted\n",
    "        if i_2 in counted_points:\n",
    "\n",
    "            # If already counted, skip remaining statements in loop and start at top of loop\n",
    "            continue\n",
    "\n",
    "        # Otherwise, continue with remaining statements\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # Get a well point by index\n",
    "        well = sc_county_wells.iloc[[i_2]]\n",
    "\n",
    "        # Reset index of well point (to 0) to match the index-reset cell\n",
    "        well = well.reset_index(drop = True)\n",
    "\n",
    "        # Check if well intersects the cell\n",
    "        # Best to use intersects instead of within or contains, as intersect will count points that fall exactly on the cell boundaries\n",
    "        # Points that fall exactly on a cell boundary (between two cells) will be allocated to the first of the two cells called in script\n",
    "        criteria_met = well.intersects(cell)[0]\n",
    "\n",
    "        # If preferred, can check if well is within cell or if cell contains well\n",
    "        # Both statements do the same thing\n",
    "        # criteria_met = well.within(cell)[0]\n",
    "        # criteria_met = cell.contains(well)[0]\n",
    "\n",
    "        # Check if criteria has been met (True)\n",
    "        if criteria_met:\n",
    "\n",
    "            # If True, increase counter by 1 for the cell\n",
    "            count += 1\n",
    "\n",
    "            # Add index of counted point to the list\n",
    "            counted_points.append(i_2)\n",
    "\n",
    "        # Otherwise, criteria is not met (False)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # Add total count for that cell to the list of counts\n",
    "    counts_list.append(count)\n",
    "\n",
    "# print(counts_list)\n",
    "\n",
    "# Add a new column to the grid GeoDataFrame with the list of counts for each cell\n",
    "sc_county_grid['Count'] = pd.Series(counts_list)\n",
    "\n",
    "# Display grid attribute table\n",
    "display(sc_county_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88647e47",
   "metadata": {},
   "source": [
    "We can check to make sure all well points in Santa Clara County were counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check total number of well points counted and compare to number of well points in input data\n",
    "print(\"Total number of well points counted: {}\\nNumber of well points in input data: {}\".format(sum(counts_list), len(sc_county_wells)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da679ea",
   "metadata": {},
   "source": [
    "Finally, we can plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4ec715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n",
    "\n",
    "# Plot data\n",
    "sc_county.plot(ax = ax, color = 'none', edgecolor = 'dimgray')\n",
    "sc_county_wells.plot(ax = ax, marker = 'o', color = 'dimgray', markersize = 3)\n",
    "sc_county_grid.plot(ax = ax, column = \"Count\", cmap = \"RdPu\", edgecolor = 'lightseagreen', linewidth = 0.5, alpha = 0.70, legend = True)\n",
    "\n",
    "# Set title\n",
    "ax.set_title('Santa Clara County - Binning Well Points', fontdict = {'fontsize': '15', 'fontweight' : '3'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5826885e",
   "metadata": {},
   "source": [
    "## Kernel Density Estimation\n",
    "\n",
    "Kernel density estimation (KDE) uses a specified kernel function to visualize the density of points or polylines. For more information on KDE, check out [this visualization](https://mathisonian.github.io/kde/).\n",
    "\n",
    "We will demonstrate three ways to perform kernel density estimation. The first way allows us to quickly visualize the KDE. The second and third ways also allow us to export and save a KDE raster for additional analysis.\n",
    "\n",
    "```{tip} We are intentionally keeping the well points beyond (but within 50 km) of the Bay Area boundaries. This provides a buffer to ensure that the KDE for wells data near the boundaries is not inadvertently influenced by these artificial county boundaries. Once KDE is run, the result can be clipped to the Bay Area boundaries (which we do in the first method).\n",
    "```\n",
    "\n",
    "### Method 1 - Display\n",
    "\n",
    "This method uses `geoplot`, a high-level plotting library for geospatial data that complements `matplotlib`. For more information on `geoplot`, check out the [documentation](https://residentmario.github.io/geoplot/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc66da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set projection to WGS 84 and reproject data\n",
    "proj_wgs = 4326\n",
    "counties_wgs = counties.to_crs(proj_wgs)\n",
    "wells_wgs = wells.to_crs(proj_wgs)\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n",
    "\n",
    "# Plot data\n",
    "counties_wgs.plot(ax = ax, color = 'none', edgecolor = 'dimgray')\n",
    "wells_wgs.plot(ax = ax, marker = 'o', color = 'dimgray', markersize = 3)\n",
    "gplt.kdeplot(wells_wgs, cmap = \"RdPu\", shade = True, clip = counties_wgs, thresh = 0, ax = ax, alpha = 0.5)\n",
    "\n",
    "# Set title\n",
    "ax.set_title('San Francisco Bay Area - Kernel Density Estimation for Wells', fontdict = {'fontsize': '15', 'fontweight' : '3'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d91ef",
   "metadata": {},
   "source": [
    "### Method 2 - Display and export with `scipy`\n",
    "\n",
    "This method uses `scipy` to visualize and export the KDE result. The `scipy` package uses a Gaussian kernel for KDE.\n",
    "\n",
    "For further reading, check out the [`scipy` documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc158c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html\n",
    "\n",
    "# Get X and Y coordinates of well points\n",
    "x_sp = wells[\"geometry\"].x\n",
    "y_sp = wells[\"geometry\"].y\n",
    "\n",
    "# Get minimum and maximum coordinate values of well points\n",
    "min_x_sp = x_sp.min()\n",
    "max_x_sp = x_sp.max()\n",
    "min_y_sp = y_sp.min()\n",
    "max_y_sp = y_sp.max()\n",
    "\n",
    "# Create a 100 by 100 cell mesh grid\n",
    "# Horizontal and vertical cell count should be the same\n",
    "XX_sp, YY_sp = np.mgrid[min_x_sp:max_x_sp:100j, min_y_sp:max_y_sp:100j]\n",
    "\n",
    "# Create 2-D array of the coordinates (paired) of each cell in the mesh grid\n",
    "positions_sp = np.vstack([XX_sp.ravel(), YY_sp.ravel()])\n",
    "\n",
    "# Create 2-D array of the coordinate values of the well points\n",
    "values_sp = np.vstack([x_sp, y_sp])\n",
    "\n",
    "# Create kernel density estimator using the values\n",
    "kde_sp = stats.gaussian_kde(values_sp)\n",
    "\n",
    "# Set kernel bandwidth (optional)\n",
    "kde_sp.set_bandwidth(0.1)\n",
    "\n",
    "# Evaluate the estimator and reshape the data to fit mesh grid\n",
    "Z_sp = np.reshape(kde_sp.evaluate(positions_sp), XX_sp.shape)\n",
    "\n",
    "# Plot data\n",
    "fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n",
    "ax.imshow(np.rot90(Z_sp), cmap = \"RdPu\", extent = [min_x_sp, max_x_sp, min_y_sp, max_y_sp])\n",
    "ax.plot(x_sp, y_sp, 'k.', markersize = 2, alpha = 0.1)\n",
    "counties.plot(ax = ax, color = 'none', edgecolor = 'dimgray')\n",
    "ax.set_title('San Francisco Bay Area - SciPy Kernel Density Estimation for Wells', fontdict = {'fontsize': '15', 'fontweight' : '3'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc265f8",
   "metadata": {},
   "source": [
    "We can export the raster if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d6713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_kde_raster(Z, XX, YY, min_x, max_x, min_y, max_y, proj, filename):\n",
    "    '''Export and save a kernel density raster.'''\n",
    "\n",
    "    # Flip array vertically and rotate 270 degrees\n",
    "    Z_export = np.rot90(np.flip(Z, 0), 3)\n",
    "\n",
    "    # Get resolution\n",
    "    xres = (max_x - min_x) / len(XX)\n",
    "    yres = (max_y - min_y) / len(YY)\n",
    "\n",
    "    # Set transform\n",
    "    transform = Affine.translation(min_x - xres / 2, min_y - yres / 2) * Affine.scale(xres, yres)\n",
    "\n",
    "    # Export array as raster\n",
    "    with rasterio.open(\n",
    "            filename,\n",
    "            mode = \"w\",\n",
    "            driver = \"GTiff\",\n",
    "            height = Z_export.shape[0],\n",
    "            width = Z_export.shape[1],\n",
    "            count = 1,\n",
    "            dtype = Z_export.dtype,\n",
    "            crs = proj,\n",
    "            transform = transform,\n",
    "    ) as new_dataset:\n",
    "            new_dataset.write(Z_export, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70839af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export raster\n",
    "export_kde_raster(Z = Z_sp, XX = XX_sp, YY = YY_sp,\n",
    "                  min_x = min_x_sp, max_x = max_x_sp, min_y = min_y_sp, max_y = max_y_sp,\n",
    "                  proj = proj, filename = \"bay-area-wells_kde_scipy.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37279196",
   "metadata": {},
   "source": [
    "### Method 3 - Display and export with `scikit-learn`\n",
    "\n",
    "This method uses `scikit-learn` to visualize and export the KDE result. Unlike the `scipy` method, we are able to specify and change various estimator parameters, including the kernel type.\n",
    "\n",
    "For further reading, check out the [`scikit-learn` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html#sklearn.neighbors.KernelDensity) and the [associated example](https://scikit-learn.org/stable/auto_examples/neighbors/plot_species_kde.html#sphx-glr-auto-examples-neighbors-plot-species-kde-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8570ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X and Y coordinates of well points\n",
    "x_sk = wells_wgs[\"geometry\"].x\n",
    "y_sk = wells_wgs[\"geometry\"].y\n",
    "\n",
    "# Get minimum and maximum coordinate values of well points\n",
    "min_x_sk = x_sk.min()\n",
    "max_x_sk = x_sk.max()\n",
    "min_y_sk = y_sk.min()\n",
    "max_y_sk = y_sk.max()\n",
    "\n",
    "# Create a 100 by 100 cell mesh grid\n",
    "# Horizontal and vertical cell count should be the same\n",
    "XX_sk, YY_sk = np.mgrid[min_x_sk:max_x_sk:100j, min_y_sk:max_y_sk:100j]\n",
    "\n",
    "# Create 2-D array of the coordinates (paired) of each cell in the mesh grid\n",
    "positions_sk = np.vstack([XX_sk.ravel(), YY_sk.ravel()]).T\n",
    "\n",
    "# Create 2-D array of the coordinate values of the well points\n",
    "Xtrain_sk = np.vstack([x_sk, y_sk]).T\n",
    "\n",
    "# Get kernel density estimator (can change parameters as desired)\n",
    "kde_sk = KernelDensity(bandwidth = 0.04, metric = 'euclidean', kernel = 'gaussian', algorithm = 'auto')\n",
    "\n",
    "# Fit kernel density estimator to wells coordinates\n",
    "kde_sk.fit(Xtrain_sk)\n",
    "\n",
    "# Evaluate the estimator on coordinate pairs\n",
    "Z_sk = np.exp(kde_sk.score_samples(positions_sk))\n",
    "\n",
    "# Reshape the data to fit mesh grid\n",
    "Z_sk = Z_sk.reshape(XX_sk.shape)\n",
    "\n",
    "# Plot data\n",
    "fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n",
    "ax.imshow(np.rot90(Z_sk), cmap = \"RdPu\", extent = [min_x_sk, max_x_sk, min_y_sk, max_y_sk])\n",
    "ax.plot(x_sk, y_sk, 'k.', markersize = 2, alpha = 0.1)\n",
    "counties_wgs.plot(ax = ax, color = 'none', edgecolor = 'dimgray')\n",
    "ax.set_title('San Francisco Bay Area - SciKit-Learn Kernel Density Estimation for Wells', fontdict = {'fontsize': '15', 'fontweight' : '3'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb70df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export raster\n",
    "export_kde_raster(Z = Z_sk, XX = XX_sk, YY = YY_sk,\n",
    "                  min_x = min_x_sk, max_x = max_x_sk, min_y = min_y_sk, max_y = max_y_sk,\n",
    "                  proj = proj_wgs, filename = \"bay-area-wells_kde_sklearn.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deaa4e2",
   "metadata": {},
   "source": [
    "There are a few other ways to compute KDE in Python. This [article](https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/) reviews and compares all these implementations."
   ]
  }
 ],
 "metadata": {
  "html_meta": {
   "description lang=en": "Learn how to aggregate geospatial data to identify areas of high and low concentration.",
   "keywords": "geospatial, aggregate, summarize, binning, vector, shapefile",
   "property=og:locale": "en_US"
  },
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   40,
   54,
   58,
   78,
   86,
   175,
   181,
   189,
   197,
   211,
   215,
   245,
   249,
   260,
   264,
   267,
   273,
   285,
   289,
   303,
   307,
   378,
   382,
   385,
   389,
   400,
   415,
   431,
   439,
   478,
   482,
   511,
   516,
   524,
   566,
   571
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}